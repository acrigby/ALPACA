"""classic Acrobot task"""
from typing import Optional

import numpy as np
from numpy import cos, pi, sin
import math

import sys

sys.path.append("/home/rigbac/Projects/ALPACA/HPC_runs/PythonScripts")

import matplotlib.pyplot as plt
from matreader import matreader, createNewInput
from PriceReader import importdict, importDNI, importdict2
import gymnasium as gym
from gymnasium import Env, spaces
from gymnasium.envs.classic_control import utils
from gymnasium.error import DependencyNotInstalled
import os
import random


__copyright__ = "Copyright 2013, RLPy http://acl.mit.edu/RLPy"
__credits__ = [
    "Alborz Geramifard",
    "Robert H. Klein",
    "Christoph Dann",
    "William Dabney",
    "Jonathan P. How",
]
__license__ = "BSD 3-Clause"
__author__ = "Christoph Dann <cdann@cdann.de>"

"""Adapted from the acrobot enviroment in Gymnasium by Aidan Rigby aidan.rigby@inl.gov to perform Dymola Optimisation"""

# SOURCE:
# https://github.com/rlpy/rlpy/blob/master/rlpy/Domains/Acrobot.py


class ReactorEcomEnv(Env):
    """
    ## Description

    This enviorment interfaces with the L2_boundaries balance of plant model found in the HYBRID library of 
    modelica models developped at Idaho National Laboratory: https://github.com/idaholab/HYBRID. The aim is 
    to find an optimised feedforward signal to provide to the feedwater coolant pump to minimise the temperature
    deviation at the steam generator exit.
    
    Details of this code can be found at {osti link}

    ## Action Space

    The action is discrete, deterministic, and represents the change in the feedforward signal applied on the FWCP.

    | Num | Action                                | Unit         |
    |-----|---------------------------------------|--------------|
    | 0   | change the feedforward signal by -0.7 | mass flow rate (kg/s) |
    | 1   | change the feedforward signal by -0.2 | mass flow rate (kg/s) |
    | 2   | change the feedforward signal by -0.1 | mass flow rate (kg/s) |
    | 3   | change the feedforward signal by  0   | mass flow rate (kg/s) |
    | 4   | change the feedforward signal by  0.1 | mass flow rate (kg/s) |
    | 5   | change the feedforward signal by  0.2 | mass flow rate (kg/s) |
    | 6   | change the feedforward signal by  0.7 | mass flow rate (kg/s) |

    ## Observation Space

    The observation is a `ndarray` with shape `(4,)` that provides information about the
    steam generator outlet temperature and power demand as well as current pump states. The states are normalised 
    to aid initialisation:

    | Num | Observation                        |Normalization       | Min                 | Max               |
    |-----|------------------------------------|--------------------|---------------------|-------------------|
    | 0   | SG Outlet Temperature              |   T_out - 673.1    | -8                  | 8                 |
    | 1   | FWCP mass flow rate                |    m_pump-58       | -10                 | 10                |
    | 2   | Turbine Electrical Power Output    | Q_e - 30 MW / 1 MW | -6                  |  6                | 
    | 3   | Pump Controller Feedforward Signal |      N/A           | -10                 | 10                |

    ## Rewards

	The reward takes the form of a scalar variable that adds a contribution to the score at each time step 
    dependent on how close the output temperature is at the end of that time step to the goal condition of 400°C. 
    This takes the form of a linear function described by equation 1.
    █(R[t]= 8-|〖(T〗_out [t]-673.15)|#1)


    ## Starting State

    The starting state is initialised using a reset file in the host repository of Starting_9900.txt

    ## Episode End

    The episode is terminated, and the score returned under two conditions. 
    1. If the SG output temperature at the end of any given time step does not lie in the 
    range: 664.15 < T_out < 682.15. This reduces the time the simulation takes to fully learn the sub space 
    by penalizing bad states. 
    2. The simulation has a run time set to 100 steps. After this the change in the 
    feedforward signal is assumed to be zero as the temperature should have returned to its nominal state

    ## Arguments

    No additional arguments are currently supported during construction.

    ```python
    import gymnasium as gym
    env = gym.make('Reactor-v3')
    ```


    ## References
    - OSTI report.
    """
    
    import threading
    
    global tid
    
    tid = threading.get_ident()

    AVAIL_Power = [4,3,2,1,0,-1,-2,-3,-4]

    actions_num = 9

    print(tid)
    
    print(os.path.abspath(os.curdir))
    ALPACApath = '/home/rigbac/Projects/ALPACA'
    
    os.mkdir('/scratch/rigbac/ThreadFilesFWHPT/' + str(tid))

    os.system('cp -a /home/rigbac/Projects/ALPACA/HPC_runs/DymolaTemplatesFWHPT/. /scratch/rigbac/ThreadFilesFWHPT/' + str(tid))

    #initialise the class and major variables
    def __init__(self, render_mode: Optional[str] = None):
        self.render_mode = render_mode
        self.screen = None
        self.clock = None
        self.isopen = True
        #set time for one step
        self.dt = 3600
        high = np.array(
            [1, 1, 1, 1, 1], dtype=np.float32
        )
        low = np.array(
            [-1,-1, -1, -1, -1], dtype=np.float32
        )
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)
        self.action_space = spaces.Discrete(9)


    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        
        self.results = {}
        self.variables = ["Time","BOP.sensorW.W","PowerDemand.y","DNI_Input.y[1]","CosEff_Input.y[1]", "BOP.deaerator.medium.p","dual_Pipe_CTES_Controlled_Feedwater.CTES.T_Ave_Conc","BOP.sensor_T2.T","SMR_Taveprogram.core.Q_total.y" ]
        for key in self.variables:
            self.results[key] = []

        price_offset = random.randint(0, 7500)
        price_year = random.randint(0, 29)

        self.price_schedule,self.prices = importdict2('/home/rigbac/Projects/ALPACA/HPC_runs/Input/test1_',price_offset,price_year) #start the function with the name of the file

        self.price_average = sum(self.prices)/len(self.prices)
        self.price_max = max(self.prices)

        os.chdir('/scratch/rigbac/ThreadFilesFWHPT/' + str(tid))
        
        super().reset(seed=seed)
        
        #define start time
        self.t = 0
        
        print(tid)
        print(os.path.abspath(os.curdir))

        oriInputFiles = ["startdsfinal.txt"]
        currentInputFiles = ["dsfinal.txt"]

        changedvars = {} 

        newinput = createNewInput(currentInputFiles, oriInputFiles, changedvars)

        matSourceFileName = "start.mat"
    
        response = matreader(matSourceFileName,self.variables)

        #get observation 

        #print(price_schedule)
        for i in range(len(self.price_schedule)):
            if self.price_schedule[i] <= self.t < self.price_schedule[i+1]:
                print(self.t)
                reward_price_t = self.prices[i]
                reward_price_t1 = self.prices[i+1]

        yout = getState(response, reward_price_t)
        
        self.state = yout

        self.original_conc_temp = yout[2]
        
        self.steps_beyond_terminated = None

        return self._get_ob(), {}

    def step(self, action):
        steps = 0
        s = self.state
        assert s is not None, "Call reset before using AcrobotEnv object."

        current_power = s[0]
        
        #chose feedforward signal based on action number
        PowerDemand = self.AVAIL_Power[action]


        #chose feedforward signal based on action number
        ramprate = 2000

        # Now, augment the state with our pump feedforward action so it can be passed to
        # dymola
        s_augmented = np.append(s, [PowerDemand, ramprate])

        #print(price_schedule)
        for i in range(len(self.price_schedule)):
            if self.price_schedule[i] <= self.t < self.price_schedule[i+1]:
                print(self.t)
                reward_price_t = self.prices[i]
                reward_price_t1 = self.prices[i+1]

        #Call main dymola physics dymola returns an observation, results vector and terminal state
        ns, terminal, p_penalty, t_penalty = DymolaDyn(self, s_augmented, reward_price_t)
        
        #print(ns[0])
        self.state = ns
        terminated = terminal
        
        reward_price = reward_price_t/self.price_average
        #print(reward_price)

        final_power = ns[0]
            
        #increment time
        self.t = self.t + 3600
            
        #Define normalise power

        average_power = ((0.5*ramprate*(final_power+current_power)) + (3600-ramprate)*final_power)/3600

        #print(average_power)
        
        #calculate reward based on linear function
        if not terminated:
            reward = (reward_price/1e8)*average_power*2 + (1 - ((ns[2]-453)**2)/10)
            if p_penalty:
                reward -= 0.1
            if t_penalty:
                reward -= 0.1
            steps += 1
            print(reward)
        elif self.steps_beyond_terminated is None:
            # Pole just fell!
            self.steps_beyond_terminated = 0
            reward = (reward_price/1e8)*average_power*2
            #os.system('rm /home/rigbac/Projects/ALPACA/HPC_runs/Output/ThreadFilesEcom/' + str(tid) +'/*')
        else:
            if self.steps_beyond_terminated == 0:
                print(
                    "You are calling 'step()' even though this "
                    "environment has already returned terminated = True. You "
                    "should always call 'reset()' once you receive 'terminated = "
                    "True' -- any further steps are undefined behavior."
                )
            self.steps_beyond_terminated += 1
            reward = 0.0
            
        return (self._get_ob(), reward, terminated, False, self.results)

    #convert state to observation array
    def _get_ob(self):
        s = self.state
        assert s is not None, "Call reset before using AcrobotEnv object."
        #self.variables = ["Time","BOP.sensorW.W","PowerDemand.y","DNI_Input.y[1]","CosEff_Input.y[1]", "BOP.deaerator.medium.p","dual_Pipe_CTES_Controlled_Feedwater.CTES.T_Ave_Conc","BOP.sensor_T2.T","SMR_Taveprogram.core.Q_total.y" ]
        return np.array(
            [(s[0]-53.35e6)/1e7, (s[1]-202000)/202000, (s[2]-423)/100, (s[3]-373)/100, s[4]/(self.price_max+1)], dtype=np.float32
        )
    

    #Unused terminal class  - can be used to calculate a more complex terminal condition
    def _terminal(self):
        s = self.state
        assert s is not None, "Call reset before using AcrobotEnv object."
        return True



def DymolaDyn(self, y0, reward_price_t):
    """
    Dymola dynamics for the time step - sets the feedforward component of the PID controller in the dsin file 
    then simulates 5 seconds of the dynamics
    """


    #generate new input

    Offset = y0[0]
    
    pd = (y0[-2]*1e6)+49e6

    dpd = pd - Offset

    duration = y0[-1]

    Offset = y0[0]

    if (pd) > 57.1e6:
        dpd  = 57.1e6 - Offset
    
    if (pd) < 48.9e6:
        dpd  = 48.9e6 - Offset

    starttime = self.t + 1003600 + 1
    
    changedvars = {'PowerDemand.offset':Offset,'PowerDemand.startTime':starttime,'PowerDemand.height':dpd, 'PowerDemand.duration':duration} 

    oriInputFiles = ["dsfinal.txt"]
    currentInputFiles = ["dsfinalnewFF.txt"]

    newinput = createNewInput(currentInputFiles, oriInputFiles, changedvars)

    #run dymosim

    os.system('./dymosim ' + newinput[0] + ' >/dev/null 2>&1')

    #get trajectory and determine if outside limits

    terminal = False

    matSourceFileName = "dsres.mat"

    response = matreader(matSourceFileName,self.variables)

    for i in range(0,len(self.variables),1):
        self.results[self.variables[i]].extend(response[self.variables[i]])

    for value in response["dual_Pipe_CTES_Controlled_Feedwater.CTES.T_Ave_Conc"]:
        if value < 449 or value > 457:
            terminal = True

    p_penalty = False
    t_penalty = False

    for value in response["BOP.sensor_T2.T"]:
        if value < 147.5 + 273.15 or value > 148.5 + 273.15:
            t_penalty = True

    for value in response["BOP.deaerator.medium.p"]:
        if value < 101000 or value > 161000:
            p_penalty = True


    #get observation 

    yout = getState(response, reward_price_t)

    return yout, terminal, p_penalty, t_penalty

def getState(response, reward_price_t):

    #print(reward_price_t)
    
    #rint(reward_price)
    #Set inital state from orginal results "BOP.sensorW.W","PowerDemand.y","DNI_Input.y[1]","CosEff_Input.y[1]", "BOP.deaerator.medium.p","dual_Pipe_CTES_Controlled_Feedwater.CTES.T_Ave_Conc","BOP.sensor_T2.T","SMR_Taveprogram.core.Q_total.y"
    #Power = response["BOP.sensorW.W"][-1]
    PowerDemand = response["PowerDemand.y"][-1]
    #dni = response["DNI_Input.y[1]"][-1]
    #CosEFF = response["CosEff_Input.y[1]"][-1]
    Deaer_P = response["BOP.deaerator.medium.p"][-1]
    Conc_Temp = response["dual_Pipe_CTES_Controlled_Feedwater.CTES.T_Ave_Conc"][-1]
    fwit = response["BOP.sensor_T2.T"][-1]
    #CorePower = response["SMR_Taveprogram.core.Q_total.y"][-1]

    
    state = [PowerDemand, Deaer_P, Conc_Temp, fwit, reward_price_t]

    #print(state)

    return state
